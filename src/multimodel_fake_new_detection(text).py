# -*- coding: utf-8 -*-
"""MultiModel Fake new detection(text).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z5R3e3D0rSti6KHo9uCEsBmvtTwQAriM

# Fake new detection project

Pipeline of this project:
1.Problem understanding

2.data colllection

3.data explore

4.data preprocessing

5.tokenizer(use keras)

6.model building & Evaluation

7. Hyparparameter tuning

8.save and deploy the model

Step 1:Problem Understanding
Problem=> classify weather given new article is fake or real

input=>text(title,content,both(title,content))

output=>Binary value(Fake/real)

# Step 2: Data collection
we are using the dataset

ISOT FAKE NEW DATASET
the ISOT Fake News Dataset contains two separate CSV files:

True.csv → Contains real news articles.

Columns: title, text

Every row represents a real news article.

Fake.csv → Contains fake news articles.

Columns: title, text

Every row represents a fake news article.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# load the datset
df_real=pd.read_csv("/content/True.csv")
df_fake=pd.read_csv("/content/Fake.csv")

df_real.head(4)

df_fake.head(5)

"""Subject / Date / Source / Author:

These are not necessary for text-based classification.

You can safely drop them before training.

They may introduce noise or bias if the model learns patterns like “all news from a certain date/source = real/fake.”
"""

# we drop the subject and date column .we are not effect the training and inference(prediction).
df_real = df_real[["title", "text"]]
df_fake = df_fake[["title", "text"]]

df_real.head(2)

"""merge the dataset into one single csv file"""

# Add labels: 1 = real, 0 = fake
df_real["label"] = 1
df_fake["label"] = 0

df_real.head(2)

df_fake.head(2)

# Combine & shuffle
df = pd.concat([df_real, df_fake], ignore_index=True)
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

df.head()

df.sample(5)

df['content'] = df['title'] + " " + df['text']
df = df[['content', 'label']]

df.head(2)

df.shape

df.info()

df.isnull().sum()

df = df.dropna()  # drop rows with NaN
df = df[df['content'].str.strip() != ""]  # remove empty strings

df.head(2)

print(df['label'].value_counts())

df['label'].value_counts().plot(kind='bar', title="Class Distribution")
plt.show()

df['label'].value_counts().plot(kind='pie', title="Class Distribution")
plt.show()

df.head(1)

print("Sample fake news:\n", df[df['label']==0].sample(1)['content'].values[0])
print("Sample real news:\n", df[df['label']==1].sample(1)['content'].values[0])

"""# Step 4: Data Preprocessing
Lowercasing

Removing punctuation, special characters, numbers

Removing extra spaces

Optional: remove stopwords
"""

def lower(text):
  text=text.lower()
  return text

df['content'] = df['content'].apply(lower)

df['content'].head(2)

import re
import string
exclude=string.punctuation
def clean_text(text):
  text=re.sub(r'\d+', '', text)
  text=text.translate(str.maketrans('','',exclude))
  return text

df['content'] = df['content'].apply(clean_text)

df['content'].sample(2)

import re
def remove_space(text):
  text = re.sub(r'\s+', ' ', text).strip()
  return text

df['content'] = df['content'].apply(remove_space)

df['content'].sample(4)

"""# Tokenization & Padding"""

df.head()

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer

"""count the vocabulary"""

vocab_size = 30000  # Limit vocabulary for speed
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")

"""apply"""

tokenizer.fit_on_texts(df['content'])

"""count the word index"""

print(tokenizer.word_index)

"""total word"""

print(tokenizer.word_counts)

print(tokenizer.word_docs)

"""number of document in the text"""

print(tokenizer.document_count)

"""to convert the text in number"""

sequence=tokenizer.texts_to_sequences(df['content'])

print(sequence[0])

print(len(sequence[0]))

print(len(sequence[1])) # length of the text

"""find the max_len this si crucial parameter to perform the padding"""

lengths = [len(seq) for seq in sequence]

# Use 95th percentile instead of max length
max_len = int(np.percentile(lengths, 95))
print("95th percentile max_len:", max_len)

"""to apply the padding to the same length input because neural network ascept the same length of input"""

from tensorflow.keras.preprocessing.sequence import pad_sequences
padded=pad_sequences(sequence,maxlen=max_len,padding='post',truncating='post')

padded[0]

print(len(padded[0]))

print(len(padded[1]))

"""the input size are same all text"""

print(len(padded))

"""to perform the train test split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(padded, df['label'], test_size=0.2, random_state=42)

print(X_train.shape,y_train.shape)

X_train

print(X_test.shape,y_test.shape)

print(X_train[0].shape)

y_test

"""# Model building"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense,Dropout,Bidirectional,GRU
from tensorflow.keras.callbacks import EarlyStopping

# vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 128

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))
model.add(Bidirectional(GRU(64, return_sequences=True)))  # keep sequences
model.add(Dropout(0.5))
model.add(Bidirectional(GRU(32, return_sequences=False)))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.build((None, max_len))  # Build the model with the input shape
model.summary()

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

callbacks = [EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True,verbose=1)]

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64, callbacks=[callbacks])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show

"""model is perform well not overfitting and underfitting"""

loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {acc*100:.4f}")

y_pred=model.predict(X_test)

print(y_pred.shape)

y_pred[0]*100

print(y_pred)

pred_classes = (y_pred > 0.5).astype("int32")

pred_classes[0]

"""array[1]=> real news

array[0]=> fake news
"""

from sklearn.metrics import confusion_matrix
conf_mat=confusion_matrix(y_test, pred_classes)

print(conf_mat)

sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""# Building the predicting system"""

text_input=input("enter the text")

text=lower(text_input)
print(text)

text=clean_text(text)
print(text)

text=remove_space(text)
print(text)

text=tokenizer.texts_to_sequences([text])
print(text)

text_seq=pad_sequences(text,maxlen=max_len,padding='post',truncating='post')
print(text_seq)

preict=model.predict(text_seq)

print(preict)

pred_classes = (preict > 0.5).astype("int32")

print(pred_classes[0][0])

if pred_classes[0][0]==1:
  print("real news")
else:
  print("fake news")

"""to create a function"""

real_news_example = "WASHINGTON (Reuters) - President Donald Trump on Wednesday nominated Judge Neil Gorsuch of the 10th U.S. Circuit Court of Appeals to the Supreme Court, picking a conservative to fill a vacancy that has been open for nearly a year."
prediction = predict_news(real_news_example)
print(f"Prediction for a real news example: {prediction}")

def predict_news(text):
  # Preprocess the text
  text = lower(text)
  text = clean_text(text)
  text = remove_space(text)

  # Convert text to sequence and pad
  text_seq = tokenizer.texts_to_sequences([text])
  text_seq = pad_sequences(text_seq, maxlen=max_len, padding='post', truncating='post')

  # Predict using the model
  prediction = model.predict(text_seq)

  # Determine the class (fake or real)
  pred_class = (prediction > 0.5).astype("int32")

  if pred_class[0][0] == 1:
    return "real news"
  else:
    return "fake news"

# Test the prediction function with examples
examples = [
    "Breaking: Scientists discover a new planet that could support human life.",
    "The local mayor inaugurated a new hospital in the downtown area today.",
    "A celebrity claims to have traveled back in time using a secret machine.",
    "The stock market saw a significant rise after the central bank reduced interest rates.",
    "Reports suggest that chocolate cures all diseases if eaten daily."
]

for i, text in enumerate(examples, 1):
    prediction = predict_news(text)
    print(f"Prediction for example {i}: {prediction}")

"""Save the model"""

model.save("fake_news_model.h5")

# 2. Save tokenizer
import pickle
with open("tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

print("✅ Model and tokenizer saved successfully!")

print("complete")

